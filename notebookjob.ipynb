{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming and etl.\n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: ade7c587-4bc3-400a-8bbd-f9c951caa395\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session ade7c587-4bc3-400a-8bbd-f9c951caa395 to get into ready status...\nSession ade7c587-4bc3-400a-8bbd-f9c951caa395 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_sales = glueContext.create_dynamic_frame.from_catalog(\n    database=\"btestohio\", \n    table_name=\"sales_csv\",\n    transformation_ctx=\"dyf_sales\"\n)\n\ndyf_products = glueContext.create_dynamic_frame.from_catalog(\n    database=\"btestohio\",\n    table_name=\"products_csv\",\n    transformation_ctx=\"dyf_products\"\n)\n\n# Verificar el n√∫mero de filas en ambas tablas\ndyf_sales_count = dyf_sales.count()\ndyf_products_count = dyf_products.count()\n\nprint(f\"Number of rows in dyf_sales: {dyf_sales_count}\")\nprint(f\"Number of rows in dyf_products: {dyf_products_count}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Number of rows in dyf_sales: 16\nNumber of rows in dyf_products: 13\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_sales.printSchema()\ndyf_products.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- transaction_id: long\n|-- store_name: string\n|-- location: string\n|-- product_id: long\n|-- quantity: long\n|-- price: double\n|-- date: string\n\nroot\n|-- product_id: long\n|-- product_name: string\n|-- category: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n\n# Convertir a DataFrame de Spark\ndf_sales = dyf_sales.toDF()\ndf_products = dyf_products.toDF()\n\n# Convertir tipos de columnas\ndf_sales = df_sales.withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\ndf_sales = df_sales.withColumn(\"price\", col(\"price\").cast(\"float\"))\n\n# Realizar el JOIN entre ambas tablas\ndf_joined = df_sales.join(df_products, on=\"product_id\", how=\"inner\")\n\n# Agregar la columna calculada\ndf_transformed = df_joined.withColumn(\"total_amount\", col(\"quantity\") * col(\"price\"))\n\n# Mostrar las primeras filas del DataFrame para ver c√≥mo quedaron los datos\ndf_transformed.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------+---------------+-----------+--------+------+----------+--------------------+-----------+------------+\n|product_id|transaction_id|     store_name|   location|quantity| price|      date|        product_name|   category|total_amount|\n+----------+--------------+---------------+-----------+--------+------+----------+--------------------+-----------+------------+\n|       102|          1006|  Digital World|    Chicago|       4|299.99|2023-01-20|      Smartphone Pro|Electronics|     1199.96|\n|       102|          1002|     Tech Haven|   New York|       7|299.99|2023-01-16|      Smartphone Pro|Electronics|     2099.93|\n|       104|          1007|     Tech Haven|   New York|       6|129.99|2023-01-21|Noise-Canceling H...|Electronics|   779.94006|\n|       104|          1005|  Digital World|    Chicago|      12|129.99|2023-01-19|Noise-Canceling H...|Electronics|   1559.8801|\n|       106|          1009|  Digital World|    Chicago|       9| 89.99|2023-01-23|   Bluetooth Speaker|Electronics|      809.91|\n|       109|          1012|  Digital World|    Chicago|       4| 39.99|2023-01-26|         USB-C Cable|Accessories|      159.96|\n|       111|          1014|Electro Central|Los Angeles|       7|299.99|2023-01-28|         DSLR Camera|Electronics|     2099.93|\n|       101|          1003|Electro Central|Los Angeles|      10|199.99|2023-01-17|      4K Ultra HD TV|Electronics|      1999.9|\n|       101|          1001|     Tech Haven|   New York|      15|199.99|2023-01-15|      4K Ultra HD TV|Electronics|     2999.85|\n|       108|          1011|Electro Central|Los Angeles|      11| 99.99|2023-01-25|      Wireless Mouse|Electronics|     1099.89|\n|       110|          1013|     Tech Haven|   New York|       5| 59.99|2023-01-27|    Portable Charger|Accessories|      299.95|\n|       107|          1010|     Tech Haven|   New York|       3|249.99|2023-01-24|              Tablet|Electronics|   749.97003|\n|       103|          1004|Electro Central|Los Angeles|       5|499.99|2023-01-18|     High-End Laptop|Electronics|     2499.95|\n|       105|          1008|Electro Central|Los Angeles|       8|349.99|2023-01-22|          Smartwatch|Electronics|     2799.92|\n|       112|          1015|  Digital World|    Chicago|       6|199.99|2023-01-29|       Action Camera|Electronics|   1199.9401|\n|       113|          1016|     Tech Haven|   New York|      10|499.99|2023-01-30|      Gaming Console|Electronics|      4999.9|\n+----------+--------------+---------------+-----------+--------+------+----------+--------------------+-----------+------------+\n\n/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\n# Convertir el DataFrame transformado a un DynamicFrame\ndyf_transformed = DynamicFrame.fromDF(df_transformed, glueContext, \"dyf_transformed\")\n\n# Escribir el resultado en S3\nglueContext.write_dynamic_frame.from_options(\n    frame = dyf_transformed,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://targetprojectco2emissions\"},\n    format = \"parquet\",\n    transformation_ctx = \"write_parquet\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f9ff1c63210>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}